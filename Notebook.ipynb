{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the data set loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\dcerc\\AppData\\Local\\Microsoft\\WindowsApps\\python3.10.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/dcerc/AppData/Local/Microsoft/WindowsApps/python3.10.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#Custom Data Set Loader to deal with multi label data-set\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "#Custom Data Set Loader to deal with multi label data-set\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            CSV_FILE (string): Path to the csv file with annotations.\n",
    "            ROOT_DIR (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on an image.\n",
    "        \"\"\"\n",
    "        #Read from the CSV file into a pandas dataframe.\n",
    "        self.annotations = pd.read_csv(csv_file) \n",
    "        self.root_dir = root_dir #Store the directory path where images are stored.\n",
    "        self.transform = transform #Store the transformation function.\n",
    "\n",
    "    #number of items in dataset\n",
    "    def __len__(self): \n",
    "        return len(self.annotations)\n",
    "\n",
    "    #retive an item from the dataset at the specified index ('idx')\n",
    "    def __getitem__(self, idx): \n",
    "        #Construct the path to the image file.\n",
    "        #image file convert it to RGB format.\n",
    "        #Extract the labels for the current image from the dataframe and convert them into a PyTorch tensor\n",
    "\n",
    "        img_path = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert('RGB') \n",
    "        labels = torch.tensor(self.annotations.iloc[idx, 1:].values.astype('float32'))\n",
    "\n",
    "        #if not None apply transformation\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        #tuple containing the transformed image and its corresponding labels.\n",
    "        return image, labels \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the model \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torchvision.models as models\n",
    "from torchvision.models import VGG16_Weights\n",
    "\n",
    "\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = F.binary_cross_entropy_with_logits(out, labels)  # Calculate loss\n",
    "        return loss\n",
    "\n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(f'Epoch: {epoch}, Last Result: {result}')\n",
    "\n",
    "\n",
    "class MusicPosterClassification(ImageClassificationBase):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MusicPosterClassification, self).__init__()\n",
    "        # Assuming we are using VGG16 as a base\n",
    "        self.vgg16 = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "        for param in self.vgg16.features.parameters():  # Freeze feature layers\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Replace the classifier part of the VGG16\n",
    "        self.vgg16.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),  # First fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),  # Second fully connected layer\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes)  # Final layer with num_classes outputs\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vgg16.features(x)  # Use the feature part of VGG16\n",
    "        # Flatten the output of the conv layers to feed into the fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # Get the output from the classifier\n",
    "        x = self.vgg16.classifier(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the actual method where we train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the actual taining function\n",
    "import torch\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "BATCH_SIZE = 50  # Adjust as needed to avoid remainder error\n",
    "THRESHOLD = 0.1\n",
    "NUM_CLASSES = 16\n",
    "CSV_FILE = 'subset_dataset.csv'\n",
    "ROOT_DIR = 'dataset4'\n",
    "MODEL_DIR = \"model\"\n",
    "LOGS_DIR = \"logs\"\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "NUM_EPOCHS: int = 30\n",
    "LEARNING_RATE: float = 0.001\n",
    "DATA_IMAGE_SIZE = (224,224)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    prob = 0\n",
    "    \n",
    "    for batch in val_loader:\n",
    "        images, labels = batch\n",
    "        output = model(images)\n",
    "        \n",
    "        loss = F.binary_cross_entropy_with_logits(output, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        prob = torch.sigmoid(output)\n",
    "        preds = (prob > THRESHOLD).float() \n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_probs.append(prob.cpu().numpy())  # Save the probabilities for ROC AUC\n",
    "\n",
    "    # Combine losses and compute metrics\n",
    "    epoch_loss = np.mean(losses)\n",
    "    all_preds = np.vstack(all_preds)\n",
    "    all_labels = np.vstack(all_labels)\n",
    "    all_probs = np.vstack(all_probs)\n",
    "    precision = precision_score(y_true=all_labels, y_pred=all_preds, average='weighted', zero_division=1)  #we changed to weighted from macro due to class imbalances\n",
    "    recall = recall_score(y_true=all_labels, y_pred=all_preds, average='weighted', zero_division=1)\n",
    "    val_score = f1_score(y_true=all_labels, y_pred= all_preds, average='weighted', zero_division=1)\n",
    "    #roc_auc = roc_auc_score(y_true=all_labels, y_score=all_probs, average='weighted', multi_class='ovr')\n",
    "    \n",
    "    output = {'val_loss': epoch_loss, 'precision': precision, 'recall': recall, 'val_score': val_score}\n",
    "    return output\n",
    "\n",
    "def fit(epochs, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    writer = SummaryWriter(LOGS_DIR)\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters())\n",
    "   \n",
    "    best_val_loss = float(0)\n",
    "    patience = 10\n",
    "    total_images_processed = 0 \n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for bnum, batch in enumerate(train_loader, start=1):\n",
    "            images, labels = batch  #vectors\n",
    "            total_images_processed += images.size(0)  # Update the counterS\n",
    "\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            train_losses.append(loss)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            print(f'Epoch [{epoch}/{epochs}], Batch [{bnum}/{len(train_loader)}], Loss: {loss.item():.4f}, Total Images Processed: {total_images_processed}')\n",
    "\n",
    "\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "\n",
    "        #Tensor Board\n",
    "        writer.add_scalar('Train Loss', result['train_loss'], epoch)\n",
    "        writer.add_scalar('Validation Loss', result['val_loss'], epoch)\n",
    "        writer.add_scalar('F1 Score', result['val_score'], epoch)\n",
    "        writer.add_scalar('Precision', result['precision'], epoch)\n",
    "        writer.add_scalar('Recall', result['recall'], epoch)\n",
    "\n",
    "        if result['val_loss'] < best_val_loss:\n",
    "            best_val_loss = result['val_loss']\n",
    "            epochs_without_improvement = 0\n",
    "            torch.save(model.state_dict(), 'best_model2.pth')\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(\"Early stopping!\")\n",
    "                torch.save(model.state_dict(), 'early_stopped_model2.pth')\n",
    "                break\n",
    "\n",
    "        checkpoint_path = os.path.join(CHECKPOINT_DIR, f'checkpoint_epoch_{epoch}.pth')\n",
    "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict(), 'best_val_loss': best_val_loss}, checkpoint_path)\n",
    "\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "\n",
    "    writer.close()\n",
    "    return history \n",
    "\n",
    "\n",
    "def TrainModel(model_output_name, train_dataloader, validation_dataloader):\n",
    "    model: MusicPosterClassification = MusicPosterClassification(NUM_CLASSES)\n",
    "    evaluation = evaluate(model, validation_dataloader)\n",
    "    print(f\"INFO: model evaluation {evaluation}\")\n",
    "\n",
    "    # history = fit(NUM_EPOCHS, LEARNING_RATE, model, train_dl, val_dl, opt_func=torch.optim.Adam)\n",
    "    history = fit(NUM_EPOCHS, model, train_dataloader, validation_dataloader, opt_func=torch.optim.Adam)\n",
    "    print(f\"INFO: model history {history}\")\n",
    "    \n",
    "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "    model_path = os.path.join(MODEL_DIR, model_output_name)\n",
    "    torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This method splits the entire dataset into 10% of it while keeping the same statistical distribution\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_stratified_subset(csv_path, subset_size=0.1, random_state=None):\n",
    "    \"\"\"\n",
    "    Creates a stratified subset of a given dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - csv_path (str): Path to the full dataset CSV file.\n",
    "    - subset_size (float): Fraction of the dataset to include in the subset (default 0.1 for 10%).\n",
    "    - random_state (int or None): Controls the shuffling applied to the data before applying the split. \n",
    "                                  Pass an int for reproducible output across multiple function calls.\n",
    "\n",
    "    Saves the subset to a new CSV file named 'subset_dataset.csv'.\n",
    "    \"\"\"\n",
    "    full_df = pd.read_csv(csv_path)\n",
    "\n",
    "    # an alternative to extract also feautres if needed\n",
    "    # features = full_df.iloc[:, 1:-1]  \n",
    "    # labels = full_df.iloc[:, -1]\n",
    "    \n",
    "    # Exclude the filename column  \n",
    "    labels = full_df.iloc[:, 1:]  \n",
    "    subset_df, _ = train_test_split(full_df, test_size=1-subset_size, stratify=labels, random_state=random_state)\n",
    "    subset_df.to_csv('subset_dataset.csv', index=False)\n",
    "\n",
    "csv_path = 'dataset_encoded.csv'\n",
    "# current set subset 0.9\n",
    "create_stratified_subset(csv_path, subset_size=0.1, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\dcerc\\AppData\\Local\\Microsoft\\WindowsApps\\python3.10.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/dcerc/AppData/Local/Microsoft/WindowsApps/python3.10.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Here we can train the model adjusting different parameters \n",
    "dataset = MultiLabelDataset(csv_file=CSV_FILE, ROOT_DIR=ROOT_DIR, transform=transforms.Compose([\n",
    "    transforms.Resize(DATA_IMAGE_SIZE), transforms.ToTensor()\n",
    "]))\n",
    "\n",
    "validation_percent = 0.2 # for example, 10%  \n",
    "num_val = int(len(dataset) * validation_percent)\n",
    "num_train = len(dataset) - num_val\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_ds, val_ds = random_split(dataset, [num_train, num_val])\n",
    "# Create DataLoaders for training and validation sets\n",
    "train_dl = DataLoader(train_ds, BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, BATCH_SIZE*2, num_workers=0, pin_memory=True)\n",
    "\n",
    "# TrainModel()\n",
    "TrainModel(\"model4.pth\", train_dl, val_dl)\n",
    "\n",
    "\n",
    "#WE TRIED TO RUN IT ON MICROSOFT AZURE AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the preprocessing of the image to        the right size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(DATA_IMAGE_SIZE),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    # Add batch dimension\n",
    "    return transform(image).unsqueeze(0) \n",
    "\n",
    "def match_classes_with_confidence(predictions, probabilities, class_names):\n",
    "    \"\"\"\n",
    "    Match the binary predictions of the model to the class names and include confidence levels,\n",
    "    rounded to 2 decimal places and formatted with a '%' sign.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions: A PyTorch tensor of shape (1, num_classes) with binary predictions.\n",
    "    - probabilities: A PyTorch tensor of shape (1, num_classes) indicating the confidence levels of predictions.\n",
    "    - class_names: A list of class names corresponding to each position in the tensors.\n",
    "\n",
    "    Returns:\n",
    "    - List of tuples with predicted class names and their confidence percentages, for those predicted,\n",
    "      formatted as 'XX.XX%'.\n",
    "    \"\"\"\n",
    "    # Flatten predictions for easier handling\n",
    "    predictions = predictions.squeeze()\n",
    "\n",
    "    # Find indices of predicted classes\n",
    "    predicted_indices = (predictions > 0).nonzero(as_tuple=False).squeeze().tolist()\n",
    "\n",
    "    # Ensure predicted_indices is a list of integers\n",
    "    if type(predicted_indices) is not list:\n",
    "        predicted_indices = [predicted_indices]  # Make it a list if it's a single number\n",
    "\n",
    "    # Extract the confidence levels for the predicted classes, round to 2.dp, and add a '%' sign\n",
    "    predicted_classes_with_confidence = [\n",
    "        (class_names[i], f\"{probabilities[0][i].item() * 100:.2f}%\") for i in predicted_indices\n",
    "    ]\n",
    "\n",
    "    return sorted(predicted_classes_with_confidence,key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'c:\\Users\\dcerc\\AppData\\Local\\Microsoft\\WindowsApps\\python3.10.exe' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/dcerc/AppData/Local/Microsoft/WindowsApps/python3.10.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "model_path = \"model\\model6.pth\"\n",
    "poster_path = \"BurningRome.jpg\"  # Replace this with the path to your poster\n",
    "model = MusicPosterClassification(num_classes=16)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "image = preprocess_image(poster_path)\n",
    "\n",
    "with torch.no_grad():  # No gradients needed\n",
    "    outputs = model(image)  # Get raw scores from the model\n",
    "    probabilities = torch.sigmoid(outputs)  # Apply sigmoid to convert scores to probabilities)\n",
    "    predictions = (probabilities > THRESHOLD).int()  # Apply threshold to get binary predictions\n",
    "\n",
    "\n",
    "# Define the class names\n",
    "class_names = [\n",
    "    \"Blues\", \"Classical\", \"Country\", \"Electronic\", \"Folk\", \"HipHop\",\n",
    "    \"Jazz\", \"LoFi\", \"Metal\", \"Pop\", \"Punk\", \"Reggae\",\n",
    "    \"Reggaeton\", \"Rock\", \"Soul\", \"Techno\"\n",
    "]\n",
    "\n",
    "# test loaded model on provided image  \n",
    "print(match_classes_with_confidence(predictions=predictions, probabilities=probabilities, class_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Gio\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl):\n",
    "    \"\"\"Plot images grid of single batch\"\"\"\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize = (16,12))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images,nrow=10).permute(1,2,0))\n",
    "        break \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MusicPosterClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchviz\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dot\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define the model with num_classes=16\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mMusicPosterClassification\u001b[49m(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Dummy input data for visualization (batch size=1, 3 channels, 224x224 size)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m dummy_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MusicPosterClassification' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Define the model with num_classes=16\n",
    "model = MusicPosterClassification(num_classes=16)\n",
    "\n",
    "# Dummy input data for visualization (batch size=1, 3 channels, 224x224 size)\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Forward pass to create the computation graph\n",
    "output = model(dummy_input)\n",
    "\n",
    "# Generate the visual diagram\n",
    "dot = make_dot(output, params=dict(model.named_parameters()))\n",
    "\n",
    "# Save the diagram as an image (PNG format)\n",
    "dot.format = 'png'\n",
    "dot.render('music_poster_classification_diagram')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
